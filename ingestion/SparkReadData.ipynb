{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT9cxddk3kqO"
      },
      "source": [
        "# Data Engineering Architecture for Large Scale Data Processing - Part I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD_ij0Jw3kVo"
      },
      "source": [
        "## Import Relevant Spark Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkJh6Q6s3cht"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pyspark import SparkConf, SparkContext, SQLContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "import databricks.koalas as ks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuHWHBA43wFF"
      },
      "source": [
        "## Spark Session Builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7BpXjio3rck"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "                    .appName(\"DevIngestApp\") \\\n",
        "                    .config('spark.port.maxRetries', 20) \\\n",
        "                    .enableHiveSupport() \\\n",
        "                    .getOrCreate() \n",
        "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")\n",
        "spark.conf.set(\"fs.azure.account.key.devstore.dfs.core.windows.net\", dbutils.secrets.get(scope = \"main-scope\", key = \"main-key\"))\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_wDJQuw5BTM"
      },
      "outputs": [],
      "source": [
        "### Environment Variables\n",
        "# Date Variable\n",
        "DATESTAMP = datetime.today().strftime(\"%Y%m%d\") ## format = '2022-03-01' for March 1, 2022\n",
        "MONTHSTAMP = datetime.today().strftime('%Y%m')  ## format = '202203'     for March 1, 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FFkNhzG49YL"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYpjNni66fS3"
      },
      "outputs": [],
      "source": [
        "def read_data(read_path, file_name):\n",
        "\n",
        "    read_schema = StructType([\n",
        "                    StructField(\"identifier\", IntType(), True),\n",
        "                    StructField(\"news_head\", StringType(), True),\n",
        "                    StructField(\"draft_date\", DateType(), True), \n",
        "                    StructField(\"air_date\", DateType(), True), \n",
        "                    StructField(\"views\", DoubleType(), True),\n",
        "                    StructField(\"comments\", StringType(), True),\n",
        "                    StructField(\"mentions\", StringType(), True),\n",
        "                    StructField(\"views_external\", DoubleType(), True),\n",
        "                    StructField(\"status\", BoolType(), True),\n",
        "                    StructField(\"headline\", BoolType(), True),\n",
        "                    StructField(\"src_file_rec\", IntegerType(), True),\n",
        "                    StructField(\"src_file_name\", StringType(), True)\n",
        "                    ])\n",
        "\n",
        "    df = spark.read.load(read_path, format=\"csv\", sep=\",\", schema=read_schema, header=\"true\")\n",
        "    file_name = file_name.split('.')[0]\n",
        "    print(\"\\nFile:\", file_name, \"Recs: \", df.count())\n",
        "    # df.printSchema()\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjAl7gpg84R1"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ve7cAvM7INV"
      },
      "outputs": [],
      "source": [
        "stage_name = \"Source\"\n",
        "flow_name = \"DailyNews\"\n",
        "source_path = '/mnt/%s/%s/' % (stage_name, flow_name)\n",
        "files = dbutils.fs.ls(source_path)\n",
        "processed_files = []\n",
        "processed_date = datetime.now().strftime('%Y-%m-%d')\n",
        "fdebug=False\n",
        "verbose=0\n",
        "for src in files:\n",
        "    init_df = spark.createDataFrame(data=[], schema=StructType([]))\n",
        "    # print(src.name, src.isDir, src.isFile, src.path, src.size)\n",
        "    file_name = (src.name.split('.')[0]).split('_')[0]\n",
        "    src_df = read_data(src.path, file_name)\n",
        "    # print(\"File:\", file_name, \"Recs: \", src_df.count())\n",
        "    dbutils.notebook.run(\"ExecuteValidationsNB\", src_df)\n",
        "    #################################################################################\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
